2022-06-26 21:47:31,938 [INFO] Running on device: cpu
2022-06-26 21:47:31,938 [INFO] Log dir: logs/mc_20_10_2_small_1_26_06_2022_21_47_31
2022-06-26 21:47:31,938 [INFO] Running problem with 20 sensors 10 targets: (checkpoint: None, seed : 123, config: configs/mc_20_10_2_small_1.yml)
2022-06-26 21:47:31,950 [INFO] Generating training dataset
2022-06-26 21:47:47,005 [INFO] Generating validation dataset
2022-06-26 21:47:48,568 [INFO] Begin training phase
2022-06-26 21:47:48,568 [INFO] Start epoch 0
2022-06-26 21:48:55,374 [INFO] 	Batch 99/10000, mean_policy_losses: 88.558, mean_net_lifetime: 9201.4228, mean_mc_travel_dist: 16232.5471, mean_rewards: 74.8132, total_rewards: 5961.9349, mean_steps: 125.8600, mean_ecr: 16.0345 mean_entropies: 2.7364, took: 66.8056s
2022-06-26 21:50:02,733 [INFO] 	Batch 199/10000, mean_policy_losses: -29.406, mean_net_lifetime: 9180.6199, mean_mc_travel_dist: 16765.4535, mean_rewards: 88.6435, total_rewards: 5834.3437, mean_steps: 131.7700, mean_ecr: 16.3446 mean_entropies: 2.6532, took: 67.3599s
2022-06-26 21:51:34,050 [INFO] 	Batch 299/10000, mean_policy_losses: 13.351, mean_net_lifetime: 12997.0299, mean_mc_travel_dist: 20067.4713, mean_rewards: 96.5125, total_rewards: 8988.4828, mean_steps: 165.2200, mean_ecr: 15.8068 mean_entropies: 2.5627, took: 91.3170s
2022-06-26 21:52:40,062 [INFO] 	Batch 399/10000, mean_policy_losses: -79.042, mean_net_lifetime: 9857.8636, mean_mc_travel_dist: 13656.8914, mean_rewards: 110.7211, total_rewards: 7129.9646, mean_steps: 121.9500, mean_ecr: 16.6699 mean_entropies: 2.6295, took: 66.0093s
2022-06-26 21:54:31,286 [INFO] 	Batch 499/10000, mean_policy_losses: -73.746, mean_net_lifetime: 16635.3504, mean_mc_travel_dist: 22339.7669, mean_rewards: 114.1659, total_rewards: 12173.0966, mean_steps: 205.6900, mean_ecr: 16.2205 mean_entropies: 2.4518, took: 111.2256s
2022-06-26 21:56:45,632 [INFO] 	Batch 599/10000, mean_policy_losses: -9.727, mean_net_lifetime: 19766.4013, mean_mc_travel_dist: 25350.2530, mean_rewards: 112.4953, total_rewards: 14701.2929, mean_steps: 243.7200, mean_ecr: 16.2810 mean_entropies: 2.4465, took: 134.3463s
2022-06-26 21:59:30,088 [INFO] 	Batch 699/10000, mean_policy_losses: -33.686, mean_net_lifetime: 24509.3069, mean_mc_travel_dist: 31228.3375, mean_rewards: 116.8221, total_rewards: 18265.8882, mean_steps: 307.6500, mean_ecr: 15.6769 mean_entropies: 2.5326, took: 164.4565s
2022-06-26 22:03:21,436 [INFO] 	Batch 799/10000, mean_policy_losses: -37.500, mean_net_lifetime: 31626.7093, mean_mc_travel_dist: 42241.4982, mean_rewards: 111.6269, total_rewards: 23179.9271, mean_steps: 417.7100, mean_ecr: 15.9044 mean_entropies: 2.4187, took: 231.3461s
2022-06-26 22:06:37,803 [INFO] 	Batch 899/10000, mean_policy_losses: 15.424, mean_net_lifetime: 29263.7571, mean_mc_travel_dist: 34719.5967, mean_rewards: 132.7592, total_rewards: 22321.9789, mean_steps: 347.4500, mean_ecr: 16.8425 mean_entropies: 2.3889, took: 196.3680s
2022-06-26 22:12:09,971 [INFO] 	Batch 999/10000, mean_policy_losses: -58.824, mean_net_lifetime: 48118.3248, mean_mc_travel_dist: 55912.9159, mean_rewards: 118.6622, total_rewards: 36939.0522, mean_steps: 559.3100, mean_ecr: 16.2060 mean_entropies: 2.2579, took: 332.1664s
2022-06-26 22:19:17,900 [INFO] 	Batch 1099/10000, mean_policy_losses: -76.233, mean_net_lifetime: 63275.4199, mean_mc_travel_dist: 67887.4872, mean_rewards: 121.3313, total_rewards: 49698.4217, mean_steps: 672.7700, mean_ecr: 15.8737 mean_entropies: 2.2072, took: 427.9294s
2022-06-26 22:26:18,493 [INFO] 	Batch 1199/10000, mean_policy_losses: 2.033, mean_net_lifetime: 61103.4235, mean_mc_travel_dist: 63674.1883, mean_rewards: 129.6589, total_rewards: 48369.1803, mean_steps: 643.7900, mean_ecr: 16.0401 mean_entropies: 2.1727, took: 420.5935s
2022-06-26 22:33:24,318 [INFO] 	Batch 1299/10000, mean_policy_losses: -90.134, mean_net_lifetime: 71240.3031, mean_mc_travel_dist: 67634.1129, mean_rewards: 129.3389, total_rewards: 57714.3385, mean_steps: 677.0900, mean_ecr: 16.3462 mean_entropies: 2.0212, took: 425.8257s
2022-06-26 22:39:20,702 [INFO] 	Batch 1399/10000, mean_policy_losses: 26.084, mean_net_lifetime: 61317.9196, mean_mc_travel_dist: 61566.4290, mean_rewards: 135.1000, total_rewards: 49004.8989, mean_steps: 625.3700, mean_ecr: 16.3632 mean_entropies: 1.9509, took: 356.3843s
2022-06-26 22:44:53,312 [INFO] 	Batch 1499/10000, mean_policy_losses: -325.681, mean_net_lifetime: 69814.6789, mean_mc_travel_dist: 65638.8858, mean_rewards: 142.9167, total_rewards: 56687.6020, mean_steps: 657.8500, mean_ecr: 16.4848 mean_entropies: 1.8377, took: 332.6109s
2022-06-26 22:50:54,107 [INFO] 	Batch 1599/10000, mean_policy_losses: -68.438, mean_net_lifetime: 66834.0283, mean_mc_travel_dist: 70522.9661, mean_rewards: 121.1159, total_rewards: 52730.0961, mean_steps: 722.3100, mean_ecr: 15.6090 mean_entropies: 1.9183, took: 360.7947s
2022-06-26 22:57:08,292 [INFO] 	Batch 1699/10000, mean_policy_losses: -72.417, mean_net_lifetime: 71884.2697, mean_mc_travel_dist: 70823.0181, mean_rewards: 133.8212, total_rewards: 57721.1846, mean_steps: 707.6300, mean_ecr: 15.7166 mean_entropies: 1.7709, took: 374.1845s
2022-06-26 23:04:20,906 [INFO] 	Batch 1799/10000, mean_policy_losses: -2.771, mean_net_lifetime: 72564.3857, mean_mc_travel_dist: 68510.2583, mean_rewards: 140.0485, total_rewards: 58863.9820, mean_steps: 691.7500, mean_ecr: 16.1615 mean_entropies: 1.7440, took: 432.6143s
2022-06-26 23:10:27,316 [INFO] 	Batch 1899/10000, mean_policy_losses: -193.904, mean_net_lifetime: 69138.4799, mean_mc_travel_dist: 68584.0188, mean_rewards: 139.2467, total_rewards: 55421.8814, mean_steps: 672.3900, mean_ecr: 16.1958 mean_entropies: 1.7459, took: 366.4103s
2022-06-26 23:17:02,871 [INFO] 	Batch 1999/10000, mean_policy_losses: -38.846, mean_net_lifetime: 71278.1903, mean_mc_travel_dist: 70809.9208, mean_rewards: 128.8488, total_rewards: 57116.9867, mean_steps: 716.5200, mean_ecr: 15.6972 mean_entropies: 1.7500, took: 395.5537s
2022-06-26 23:22:20,871 [INFO] 	Batch 2099/10000, mean_policy_losses: -182.770, mean_net_lifetime: 66544.2522, mean_mc_travel_dist: 65085.8117, mean_rewards: 150.5569, total_rewards: 53527.5410, mean_steps: 630.6300, mean_ecr: 16.8402 mean_entropies: 1.5684, took: 318.0013s
2022-06-26 23:27:52,294 [INFO] 	Batch 2199/10000, mean_policy_losses: -17.455, mean_net_lifetime: 73237.3825, mean_mc_travel_dist: 66706.5205, mean_rewards: 152.6311, total_rewards: 59896.8933, mean_steps: 651.7000, mean_ecr: 16.8091 mean_entropies: 1.5629, took: 331.4229s
