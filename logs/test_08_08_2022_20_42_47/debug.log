2022-08-08 20:42:47,882 [INFO] Running on device: cuda
2022-08-08 20:42:47,882 [INFO] Log dir: logs/test_08_08_2022_20_42_47
2022-08-08 20:42:47,883 [INFO] Running problem with 20 sensors 10 targets: (checkpoint: None, seed : 123, config: configs/test.yml)
2022-08-08 20:43:49,020 [INFO] 	Process 0 - batch 99: mean_policy_losses: 476.635, mean_net_lifetime: 2869.6788, mean_mc_travel_dist: 1906.0220, mean_rewards: 162.6631, total_rewards: 1040.1138, mean_steps: 16.4500, mean_ecr: 0.0416 mean_entropies: 2.9919, took: 38.0892s
2022-08-08 20:43:53,473 [INFO] 	Process 1 - batch 99: mean_policy_losses: 442.256, mean_net_lifetime: 3054.7739, mean_mc_travel_dist: 2006.6961, mean_rewards: 164.8739, total_rewards: 1130.8076, mean_steps: 17.3200, mean_ecr: 0.0416 mean_entropies: 2.9892, took: 39.5067s
2022-08-08 20:44:02,279 [INFO] 	Process 2 - batch 99: mean_policy_losses: 243.048, mean_net_lifetime: 3012.0608, mean_mc_travel_dist: 2004.6846, mean_rewards: 157.6978, total_rewards: 1073.6080, mean_steps: 17.6600, mean_ecr: 0.0414 mean_entropies: 2.9842, took: 44.8379s
2022-08-08 20:44:03,289 [INFO] 	Process 3 - batch 99: mean_policy_losses: 259.202, mean_net_lifetime: 3068.3216, mean_mc_travel_dist: 1997.5114, mean_rewards: 159.1238, total_rewards: 1136.8665, mean_steps: 17.7600, mean_ecr: 0.0416 mean_entropies: 2.9833, took: 42.8963s
2022-08-08 20:44:09,581 [INFO] 	Process 4 - batch 99: mean_policy_losses: 170.810, mean_net_lifetime: 3032.9268, mean_mc_travel_dist: 2006.4130, mean_rewards: 159.8458, total_rewards: 1107.7618, mean_steps: 17.7000, mean_ecr: 0.0414 mean_entropies: 2.9798, took: 46.0060s
2022-08-08 20:44:09,784 [INFO] 	Process 5 - batch 99: mean_policy_losses: 88.379, mean_net_lifetime: 2921.8099, mean_mc_travel_dist: 1945.0482, mean_rewards: 160.4886, total_rewards: 1039.7241, mean_steps: 17.4000, mean_ecr: 0.0413 mean_entropies: 2.9785, took: 43.4319s
2022-08-08 20:44:32,971 [INFO] 	Process 0 - batch 199: mean_policy_losses: 8.357, mean_net_lifetime: 3275.9031, mean_mc_travel_dist: 2135.9952, mean_rewards: 157.4817, total_rewards: 1210.5651, mean_steps: 19.4600, mean_ecr: 0.0400 mean_entropies: 2.9684, took: 43.9507s
2022-08-08 20:44:35,706 [INFO] 	Process 1 - batch 199: mean_policy_losses: -4.217, mean_net_lifetime: 3100.7408, mean_mc_travel_dist: 2061.6397, mean_rewards: 156.2136, total_rewards: 1109.1609, mean_steps: 18.6400, mean_ecr: 0.0401 mean_entropies: 2.9675, took: 42.2333s
2022-08-08 20:44:41,534 [INFO] 	Process 2 - batch 199: mean_policy_losses: 20.315, mean_net_lifetime: 3130.3982, mean_mc_travel_dist: 2055.0808, mean_rewards: 156.0956, total_rewards: 1140.1092, mean_steps: 18.6600, mean_ecr: 0.0402 mean_entropies: 2.9662, took: 39.2545s
2022-08-08 20:44:44,172 [INFO] 	Process 3 - batch 199: mean_policy_losses: -19.776, mean_net_lifetime: 2991.9390, mean_mc_travel_dist: 2018.8604, mean_rewards: 157.4740, total_rewards: 1041.5894, mean_steps: 17.7100, mean_ecr: 0.0400 mean_entropies: 2.9663, took: 40.8829s
2022-08-08 20:44:49,668 [INFO] 	Process 5 - batch 199: mean_policy_losses: 82.235, mean_net_lifetime: 3305.7208, mean_mc_travel_dist: 2117.4809, mean_rewards: 163.6109, total_rewards: 1254.9633, mean_steps: 19.2900, mean_ecr: 0.0398 mean_entropies: 2.9655, took: 39.8843s
2022-08-08 20:44:51,198 [INFO] 	Process 4 - batch 199: mean_policy_losses: 76.914, mean_net_lifetime: 3296.3189, mean_mc_travel_dist: 2146.8620, mean_rewards: 157.5111, total_rewards: 1237.4750, mean_steps: 19.4500, mean_ecr: 0.0399 mean_entropies: 2.9646, took: 41.6185s
2022-08-08 20:45:15,194 [INFO] 	Process 0 - batch 299: mean_policy_losses: 136.125, mean_net_lifetime: 3195.2393, mean_mc_travel_dist: 2041.2958, mean_rewards: 159.7544, total_rewards: 1207.8599, mean_steps: 18.8600, mean_ecr: 0.0398 mean_entropies: 2.9561, took: 42.2232s
2022-08-08 20:45:17,600 [INFO] 	Process 1 - batch 299: mean_policy_losses: 155.618, mean_net_lifetime: 3301.7985, mean_mc_travel_dist: 2076.5692, mean_rewards: 161.0770, total_rewards: 1283.3912, mean_steps: 19.3100, mean_ecr: 0.0397 mean_entropies: 2.9551, took: 41.8942s
2022-08-08 20:45:23,314 [INFO] 	Process 2 - batch 299: mean_policy_losses: 190.292, mean_net_lifetime: 3575.5799, mean_mc_travel_dist: 2198.4193, mean_rewards: 164.0040, total_rewards: 1448.4581, mean_steps: 20.2800, mean_ecr: 0.0394 mean_entropies: 2.9541, took: 41.7807s
2022-08-08 20:45:27,269 [INFO] 	Process 3 - batch 299: mean_policy_losses: 144.437, mean_net_lifetime: 3284.2797, mean_mc_travel_dist: 2066.2016, mean_rewards: 160.1207, total_rewards: 1286.1541, mean_steps: 19.0400, mean_ecr: 0.0397 mean_entropies: 2.9535, took: 43.0969s
2022-08-08 20:45:31,786 [INFO] 	Process 5 - batch 299: mean_policy_losses: 167.626, mean_net_lifetime: 3563.0235, mean_mc_travel_dist: 2210.6307, mean_rewards: 165.6178, total_rewards: 1416.8315, mean_steps: 20.1200, mean_ecr: 0.0397 mean_entropies: 2.9530, took: 42.1181s
2022-08-08 20:45:32,414 [INFO] 	Process 4 - batch 299: mean_policy_losses: 150.163, mean_net_lifetime: 3292.2842, mean_mc_travel_dist: 2069.2510, mean_rewards: 159.0640, total_rewards: 1289.0665, mean_steps: 19.4100, mean_ecr: 0.0396 mean_entropies: 2.9529, took: 41.2156s
2022-08-08 20:45:58,234 [INFO] 	Process 0 - batch 399: mean_policy_losses: 38.667, mean_net_lifetime: 3294.7353, mean_mc_travel_dist: 2089.4289, mean_rewards: 164.5593, total_rewards: 1274.5136, mean_steps: 18.9400, mean_ecr: 0.0393 mean_entropies: 2.9406, took: 43.0397s
2022-08-08 20:46:02,103 [INFO] 	Process 1 - batch 399: mean_policy_losses: 50.355, mean_net_lifetime: 3446.5422, mean_mc_travel_dist: 2171.9216, mean_rewards: 159.3157, total_rewards: 1345.7901, mean_steps: 19.9400, mean_ecr: 0.0392 mean_entropies: 2.9375, took: 44.5036s
2022-08-08 20:46:05,024 [INFO] 	Process 2 - batch 399: mean_policy_losses: 37.433, mean_net_lifetime: 3358.1938, mean_mc_travel_dist: 2114.4183, mean_rewards: 162.6948, total_rewards: 1321.7884, mean_steps: 19.5400, mean_ecr: 0.0392 mean_entropies: 2.9324, took: 41.7098s
2022-08-08 20:46:13,992 [INFO] 	Process 3 - batch 399: mean_policy_losses: 51.992, mean_net_lifetime: 3690.6310, mean_mc_travel_dist: 2220.1247, mean_rewards: 166.4894, total_rewards: 1529.8197, mean_steps: 21.2300, mean_ecr: 0.0388 mean_entropies: 2.9232, took: 46.7229s
2022-08-08 20:46:16,964 [INFO] 	Process 4 - batch 399: mean_policy_losses: -36.811, mean_net_lifetime: 3430.6213, mean_mc_travel_dist: 2166.8985, mean_rewards: 155.8146, total_rewards: 1329.4881, mean_steps: 20.7000, mean_ecr: 0.0394 mean_entropies: 2.9166, took: 44.5498s
2022-08-08 20:46:18,763 [INFO] 	Process 5 - batch 399: mean_policy_losses: 147.793, mean_net_lifetime: 3881.9399, mean_mc_travel_dist: 2319.4043, mean_rewards: 164.9926, total_rewards: 1629.5079, mean_steps: 22.2200, mean_ecr: 0.0390 mean_entropies: 2.9142, took: 46.9773s
